# -*- coding: utf-8 -*-
"""CovidTransferLearning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ROcVCQclUuWkY-RgUpNJvKbzLRCLoyxq
"""

# importing required libraries
from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array
from keras.models import Sequential, Model
from keras.layers import Conv2D, MaxPooling2D , Dense , Dropout , Flatten,  Input, BatchNormalization
from tensorflow.keras.applications import ResNet50
from sklearn.model_selection import train_test_split, StratifiedKFold
from tensorflow.keras.applications.vgg16 import preprocess_input 
from tensorflow.keras.applications import VGG16
from glob import glob 
import matplotlib.pyplot as plt 
from matplotlib.image import imread
from matplotlib import pyplot
import cv2 
import numpy as np 
import pandas as pd 
import tensorflow as tf 
import os
import random
import shutil

# mounting google drive 
from google.colab import drive
drive.mount('/content/drive')

# Uncomment one of the below depending on the requirement

# download dataset folder from web 
!wget 'http://www.cse.fau.edu/~xqzhu/courses/cot6930/dataset/covidImages.zip'

# OR copying dataset zipped folder from drive 
# modify the folder locations depending on system specific folder location
#!cp '/content/drive/My Drive/Upwork/covid_images_dataset/covidImages.zip'  '/content/'

"""
import shutil
shutil.rmtree('/content/COVID-19_Radiography_Dataset/')
"""

!unzip covidImages.zip

# defining image folders
base_folder =  '/content/COVID-19_Radiography_Dataset/'
covid_subfolder = base_folder + 'COVID'
lung_opacity_subfolder = base_folder + 'Lung_Opacity'
normal_subfolder = base_folder + 'Normal'
viral_subfolder   = base_folder + 'Viral Pneumonia'

# finding filenames in respective subfolders
covid_filenames = glob(covid_subfolder + '/*')
lung_opacity_filenames = glob(lung_opacity_subfolder + '/*')
normal_filenames = glob(normal_subfolder + '/*')
viral_filenames = glob(viral_subfolder + '/*')

# printing filename info 
print("Total {} images found in {} folder".format(len(covid_filenames), covid_subfolder))
print("Total {} images found in {} folder".format(len(lung_opacity_filenames), lung_opacity_subfolder))
print("Total {} images found in {} folder".format(len(normal_filenames), normal_subfolder))
print("Total {} images found in {} folder".format(len(viral_filenames), viral_subfolder))

classes = ['Covid','Lung_Opacity','Normal','Viral Pneumonia']
folders = [covid_subfolder,lung_opacity_subfolder,normal_subfolder,viral_subfolder]
for i in range(4):
  training = base_folder + '/train/' + classes[i] + '/'
  validation = base_folder + '/validation/' + classes[i] + '/'
  test = base_folder + '/test/' + classes[i] + '/'
  #files = os.listdir(folders[i])

  if not os.path.exists(training): # create a tempory folder 'preview' to save generated images
    os.makedirs(training)
  if not os.path.exists(validation): # create a tempory folder 'preview' to save generated images
    os.makedirs(validation)
  if not os.path.exists(test): # create a tempory folder 'preview' to save generated images
    os.makedirs(test)

  for m in range(800):
    files = [filenames for (filenames) in os.listdir(folders[i])]
    random_file = random.choice(files)
    shutil.move(os.path.join(folders[i], random_file), training)
  for m in range(250):
    files = [filenames for (filenames) in os.listdir(folders[i])]
    random_file = random.choice(files)
    shutil.move(os.path.join(folders[i], random_file), validation)    
  for m in range(250):
    files = [filenames for (filenames) in os.listdir(folders[i])]
    random_file = random.choice(files)
    shutil.move(os.path.join(folders[i], random_file), test)

# Defining ImageDataGenerators for train, val and test sets 
train_data_generator = ImageDataGenerator( rotation_range=40, width_shift_range=0.2, height_shift_range=0.2, rescale=1./255,
                        shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest')
val_data_generator   =  ImageDataGenerator(rescale=1./255)
test_data_generator  =  ImageDataGenerator(rescale=1./255)

# Defining image resolution, batch size, etc 
IMAGE_WIDTH=150
IMAGE_HEIGHT=150
BATCH_SIZE=30

# import vgg16 model 
from tensorflow.python.keras.applications.resnet import ResNet50
# load model
model=ResNet50()
# summarize the model
model.summary()

from keras.layers import Input
import random
pic      = random.choice(os.listdir(base_folder + 'train/Covid/'))
fileName = base_folder + 'train/Covid/' + pic
image    = load_img(fileName, target_size=(224,224),color_mode="rgb")#"rgb" for color mode; "grayscale"
pyplot.imshow(image)
pyplot.show()
photo=img_to_array(image)# convert to numpy array
photo.shape

# now we need to reshape to image to VGG16 required input format
photo = photo.reshape((1, photo.shape[0], photo.shape[1], photo.shape[2]))
photo.shape

ypred=model.predict(photo)
print(ypred)

from keras.applications.vgg16 import decode_predictions
# convert the probabilities to class labels
label = decode_predictions(ypred)
print(label)
# retrieve the most likely result, e.g. highest probability
label = label[0][0]
# print the classification
print('%s(%.2f%%)'%(label[1], label[2]*100))
pyplot.imshow(image)
pyplot.show()

# load model and specify a new input shape for images
new_input = Input(shape=(IMAGE_WIDTH, IMAGE_HEIGHT,3))
model     = ResNet50(include_top=False, input_tensor=new_input)
model.summary()

# define finetune cnn model
from keras.models import Model
def finetuning_model():
  # load model
  model = ResNet50(include_top=False, input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT,3))
  # mark loaded layers as not trainable
  for layer in model.layers:
    layer.trainable=False
  # add new classifier layers
  flat1=Flatten()(model.layers[-1].output)
  class1=Dense(128, activation='relu',kernel_initializer='he_uniform')(flat1)
  #bn = BatchNormalization()(flatten)
  #dropout = Dropout(0.5)(bn)
  num_output_neurons = 4
  output=Dense(num_output_neurons, activation='softmax')(class1)
  # define new model
  model = Model(inputs=model.inputs, outputs=output)
  # compile model
  model.compile(loss="categorical_crossentropy",optimizer="rmsprop",metrics=["accuracy"])
  
  return model

modelResNet=finetuning_model()
modelResNet.summary()

training_data_generator=ImageDataGenerator(
    rotation_range=40,
    width_shift_range=0.2,
    height_shift_range=0.2,
    rescale=1./255,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest')
validation_data_generator=ImageDataGenerator(rescale=1./255)
test_data_generator=ImageDataGenerator(rescale=1./255)

training_data_dir = base_folder+'train/'
validation_data_dir = base_folder+'validation/'
test_data_dir = base_folder+'test/'
IMAGE_WIDTH=150
IMAGE_HEIGHT=150
BATCH_SIZE=20
training_generator = training_data_generator.flow_from_directory(
    training_data_dir,
    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),
    batch_size=BATCH_SIZE,
    class_mode="categorical")
validation_generator=validation_data_generator.flow_from_directory(
    validation_data_dir,
    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),
    batch_size=BATCH_SIZE,
    class_mode='categorical')
test_generator=test_data_generator.flow_from_directory(
    test_data_dir,
    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),
    batch_size=1,class_mode="categorical",
    shuffle=False)

EPOCHS  = 10
history = modelResNet.fit(
    training_generator,
    steps_per_epoch=len(training_generator.filenames) //BATCH_SIZE,
    epochs=EPOCHS,
    validation_data=validation_generator,
    validation_steps=len(validation_generator.filenames)//BATCH_SIZE)

# plotting loss and accuracy results 

import matplotlib.pyplot as plt
fig, ax = plt.subplots(1, 2, figsize=(18,6))

# summarize history for accuracy
ax[0].plot(history.history['accuracy'])
ax[0].plot(history.history['val_accuracy'])
ax[0].set_title('Model Accuracy')
ax[0].set_ylabel('accuracy')
ax[0].set_xlabel('epoch')
ax[0].legend(['train', 'test'], loc='upper left')

# summarize history for loss
ax[1].plot(history.history['loss'])
ax[1].plot(history.history['val_loss'])
ax[1].set_title('Model Loss')
ax[1].set_ylabel('loss')
ax[1].set_xlabel('epoch')
ax[1].legend(['train', 'test'], loc='upper right')

plt.show()

training_data_dir   = base_folder +'train/'
validation_data_dir = base_folder +'validation/'
test_data_dir       = base_folder +'test/'
IMAGE_WIDTH=150
IMAGE_HEIGHT=150
BATCH_SIZE=20

test_generator = test_data_generator.flow_from_directory(
    test_data_dir,
    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),
    batch_size=1,
    class_mode="categorical",
    shuffle=False)

# Check classification accuracy on the test set
_, acc=modelResNet.evaluate_generator(test_generator, steps=len(test_generator),verbose=0)
print('Test Accuracy:%.3f%%'%(acc*100.0))

# import vgg16 model 
from keras.applications.vgg16 import VGG16
# load model
model=VGG16()
# summarize the model
model.summary()

# import vgg16 model
from keras.applications.vgg16 import VGG16
# load model and specify a new input shape for images
new_input = Input(shape=(IMAGE_WIDTH, IMAGE_HEIGHT,3))
model     = VGG16(include_top=False, input_tensor=new_input)
model.summary()

# define finetune cnn model
from keras.models import Model
def finetuning_model():
  # load model
  model = VGG16(include_top=False, input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT,3))
  # mark loaded layers as not trainable
  for layer in model.layers:
    layer.trainable=False
  # add new classifier layers
  flat1=Flatten()(model.layers[-1].output)
  class1=Dense(128, activation='relu',kernel_initializer='he_uniform')(flat1)
  #bn = BatchNormalization()(flatten)
  #dropout = Dropout(0.5)(bn)
  num_output_neurons = 4
  output=Dense(num_output_neurons, activation='softmax')(class1)
  # define new model
  model = Model(inputs=model.inputs, outputs=output)
  # compile model
  model.compile(loss="categorical_crossentropy",optimizer="rmsprop",metrics=["accuracy"])
  return model

modelVGG=finetuning_model()
modelVGG.summary()

EPOCHS  = 10
history = modelVGG.fit(
    training_generator,
    steps_per_epoch=len(training_generator.filenames) //BATCH_SIZE,
    epochs=EPOCHS,
    validation_data=validation_generator,
    validation_steps=len(validation_generator.filenames)//BATCH_SIZE)

# plotting loss and accuracy results 

import matplotlib.pyplot as plt
fig, ax = plt.subplots(1, 2, figsize=(18,6))

# summarize history for accuracy
ax[0].plot(history.history['accuracy'])
ax[0].plot(history.history['val_accuracy'])
ax[0].set_title('Model Accuracy')
ax[0].set_ylabel('accuracy')
ax[0].set_xlabel('epoch')
ax[0].legend(['train', 'test'], loc='upper left')

# summarize history for loss
ax[1].plot(history.history['loss'])
ax[1].plot(history.history['val_loss'])
ax[1].set_title('Model Loss')
ax[1].set_ylabel('loss')
ax[1].set_xlabel('epoch')
ax[1].legend(['train', 'test'], loc='upper right')

plt.show()

training_data_dir   = base_folder +'train/'
validation_data_dir = base_folder +'validation/'
test_data_dir       = base_folder +'test/'
IMAGE_WIDTH=150
IMAGE_HEIGHT=150
BATCH_SIZE=20

test_generator = test_data_generator.flow_from_directory(
    test_data_dir,
    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),
    batch_size=1,
    class_mode="categorical",
    shuffle=False)

# Check classification accuracy on the test set
_, acc=modelVGG.evaluate_generator(test_generator, steps=len(test_generator),verbose=0)
print('Test Accuracy:%.3f%%'%(acc*100.0))

# creating a dataframe for containing the filepaths (complete absolute filepaths) and corresponding class labels
label_names   = ['covid','lung_opacity','normal','viral']
all_filepaths = covid_filenames + lung_opacity_filenames + normal_filenames + viral_filenames
all_labels    = [label_names[0]]*len(covid_filenames) + [label_names[1]]*len(lung_opacity_filenames) + [label_names[2]]*len(normal_filenames) + [label_names[3]]*len(viral_filenames) 
df = pd.DataFrame({'filename':all_filepaths, 'class':all_labels})
print(df.head())
print(df.describe())

X = df['filename']
y = df['class']

from mlxtend.evaluate import paired_ttest_5x2cv
# check if difference between algorithms is real (two knn classifiers)
t, p = paired_ttest_5x2cv(estimator1=modelResNet, estimator2=modelVGG, X=test_generator.image_data_generator, y=test_generator.labels, scoring='accuracy')
# summarize
print('t-Statistic:%.3f, P-value:%.3f'%(t, p))
# interpret the result
if p <= 0.05:
  print('Difference between mean performance is probably real')
else:
  print('Algorithms probably have the same performance')

test_generator.filepaths

# check if difference between algorithms is real (two knn classifiers)
t, p = paired_ttest_5x2cv(estimator1=modelResNet, estimator2=modelVGG, test_generator, scoring='accuracy')
# summarize
print('t-Statistic:%.3f, P-value:%.3f'%(t, p))
# interpret the result
if p <= 0.05:
  print('Difference between mean performance is probably real')
else:
  print('Algorithms probably have the same performance')